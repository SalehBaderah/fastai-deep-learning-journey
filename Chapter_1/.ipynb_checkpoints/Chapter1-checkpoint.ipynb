{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44a6d0e4-9e35-4024-8ee1-3a0a0b1d6626",
   "metadata": {},
   "source": [
    "# Chapter 1: Deep Learning Fundamentals\n",
    "\n",
    "## 1. What is Deep Learning?\n",
    "> **Definition:** A computer technique used to extract and transform data for use cases such as human speech recognition, animal image classification, etc.\n",
    "\n",
    "It works by using multiple layers of **Neural Networks**. Each of these layers takes input from the previous layer and refines it by minimizing errors and improving accuracy.\n",
    "\n",
    "### Common Tasks of Deep Learning\n",
    "* **NLP (Natural Language Processing):** Answering questions, speech recognition, summarizing documents, classifying text, finding names/dates in documents, and searching for concepts.\n",
    "* **Computer Vision:** Satellite and drone imagery interpretation (e.g., disaster resilience), face recognition, image captioning, reading traffic signs, and locating pedestrians/vehicles for autonomous driving.\n",
    "* **Medicine:** Finding anomalies in radiology images (CT, MRI, X-ray), counting features in pathology slides, measuring features in ultrasounds, and diagnosing diabetic retinopathy.\n",
    "* **Image Generation:** Colorizing black-and-white images, increasing image resolution, removing noise, or converting images into the style of famous artists.\n",
    "* **Other Applications:** Financial and logistical forecasting, text-to-speech, and much more.\n",
    "\n",
    "### The Goal\n",
    "To build a machine capable of learning from data to perform a task (training) or controlling a system.\n",
    "\n",
    "## 2. Tools\n",
    "* **Jupyter Notebooks:** The interactive coding environment.\n",
    "* **fastai:** The high-level library for rapid implementation.\n",
    "* **PyTorch:** The underlying deep learning framework.\n",
    "* **[Book Website](https://course.fast.ai/Lessons/lesson1.html):** Source for code and resources.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Core Concepts\n",
    "\n",
    "### What is Machine Learning?\n",
    "Allowing a computer or algorithm to learn from data rather than being explicitly programmed with rules.\n",
    "\n",
    "### What is a Neural Network?\n",
    "A mathematical function that mimics the human brain. It uses **weights** that are optimized automatically using **Stochastic Gradient Descent (SGD)** (covered in Chapter 4) to find the optimal accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Terminology & Definitions\n",
    "\n",
    "### Data Splits\n",
    "* **Training Set (~80%):** Used for **training** the model.\n",
    "* **Validation Set (~20%):** Used to measure the **accuracy** of the model (i.e., \"How well is the model doing on data it hasn't seen?\").\n",
    "    * **Seed:** A random number generator setting (e.g., `seed=42`). We set this to ensure we get the *same* validation set every time we run the code, ensuring reproducibility.\n",
    "\n",
    "### Model Behavior\n",
    "* **Overfitting:** When the model \"memorizes\" the specific images in the training set rather than learning the **underlying patterns**. This leads to poor performance on the validation set.\n",
    "* **Metric:** A function used to measure the quality of the model's predictions. It is calculated and displayed at the end of each **epoch** (e.g., Accuracy, Error Rate).\n",
    "* **Epoch:** One complete pass through all the images in the dataset.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/overfitting_example.png\" width=\"400\" alt=\"Overfitting Graph\">\n",
    "</div>\n",
    "\n",
    "### Transfer Learning\n",
    "* **Transfer Learning:** When we use a **Pretrained Model** for a task different from what it was originally trained for.\n",
    "* **Fine-Tuning:** The process of training a pretrained model for additional epochs on your specific dataset to adapt the weights to your new task.\n",
    "\n",
    "#### Code Example:\n",
    "```python\n",
    "# 'dls' is your DataLoaders\n",
    "# 'resnet34' is the pretrained model architecture\n",
    "# 'error_rate' is the metric we want to see\n",
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "# Fine-tune the model for 1 epoch \n",
    "learn.fine_tune(1)\n",
    "    \n",
    "```\n",
    "---\n",
    "# Recap\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/overfitting_example.png\" width=\"400\" alt=\"Overfitting Graph\">\n",
    "</div>\n",
    "\n",
    "\n",
    "# Validation Sets & Test Sets\n",
    "The problem with traditional validation set is the `Data Leakage` hapens when we train the model more than once and update the hyperparameter until we find the best validation score.\n",
    "- Round 1: You try a \"Learning Rate\" of 0.1. Validation accuracy is 80%.\n",
    "\n",
    "- Round 2: You try a \"Learning Rate\" of 0.01. Validation accuracy is 82%.\n",
    "\n",
    "- Round 3: You try a \"Learning Rate\" of 0.001. Validation accuracy is 85%.\n",
    "\n",
    "  By choosing the settings that performed best on validation set, we indirectly \"LEAKED\" information from the validation set. The model now is Biased toward it .\n",
    "\n",
    "The solution : The `Test set`\n",
    "this set is totaly hidden, It cannot be used to improve the model, Used only to evaluate the model at the end.\n",
    "> Both validation set and Test set should have enough data to ensure that we get good estimate of the accuracy\n",
    "> In case we don't have enough data we may need just validation set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
