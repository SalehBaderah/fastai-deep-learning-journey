{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d3c40-ecae-4e4f-8efb-71f3da3cb574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef366a6-b5af-4100-85fb-9516481b3941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc4b50-6b16-4885-b459-f938e24e97cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c91a4f-51d5-4246-af6e-6af149ef649f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48322f6a-1602-49c6-8a95-88e4dce98f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d9237f-2b96-49cf-9b9f-6a55cd53380e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5adc59-2b06-4fbd-b546-a22439f008a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c3f3c-62ac-4b46-8e9f-8bd1330c3fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d91ef-f9c2-42b9-8bd2-b104d08b1f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94068a9a-7b43-43fe-a121-de898e53d3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc6ca2-3edc-4f9c-b32a-201c7198517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9225b3e-9951-4492-9530-5b4f934418fc",
   "metadata": {},
   "source": [
    "# Chapter 3 : Data Ethics\n",
    "---\n",
    "\n",
    "### The Core Argument\n",
    "The authors argue that machine learning models are not objective or neutral. Because they are trained on data created by humans, they often reflect human mistakes and prejudices. Since deep learning is used at a massive scale (affecting millions of people), small errors or biases in the code can cause significant harm in the real world.\n",
    "\n",
    "### 1. Key Ethical Issues\n",
    "The chapter categorizes the main problems into three areas:\n",
    "\n",
    "* **Feedback Loops:**\n",
    "    This occurs when an algorithm's predictions change the real world, which then creates new data that reinforces the original prediction.\n",
    "    * *Example:* YouTube's recommendation engine tried to maximize \"watch time.\" It learned that extreme content kept people glued to the screen, so it recommended more of it. This radicalized users, which created more data suggesting they *wanted* extreme content, creating a vicious cycle.\n",
    "\n",
    "* **Bias:**\n",
    "    Algorithms often reproduce existing societal problems because they learn from imperfect data.\n",
    "    * **Historical Bias:** If historical data reflects racism or sexism (e.g., higher arrest rates for certain groups due to systemic issues), the model will learn to make decisions based on those disparities.\n",
    "    * **Representation Bias:** If the training data doesn't represent the whole population, the model will fail for the underrepresented groups. (e.g., ImageNet having mostly Western images, causing models to fail at recognizing objects in developing countries).\n",
    "    * **Measurement Bias:** This happens when we measure a *proxy* instead of the real thing. (e.g., predicting \"who will have a stroke\" based on \"who visited the doctor.\" This ignores people who had strokes but couldn't afford a doctor).\n",
    "\n",
    "* **Disinformation:**\n",
    "    Deep learning makes it easy to generate fake text and images (deepfakes) cheaply and quickly. This allows bad actors to flood the internet with false information, eroding trust in shared reality.\n",
    "\n",
    "### 2. Why Do These Problems Happen?\n",
    "* **Metric Fixation:** Companies often optimize for a single number (like clicks, engagement, or speed) and ignore the social impact or \"externalities.\"\n",
    "* **Homogeneous Teams:** The tech industry largely consists of people from similar backgrounds (often young, white, male). These teams have \"blind spots\"â€”they often don't anticipate how a product might harm women, minorities, or older people because they don't share those life experiences.\n",
    "* **Lack of Context:** Engineers often build models in a bubble without understanding the complex social environment where the model will actually be deployed.\n",
    "\n",
    "### 3. How to Fix It\n",
    "The authors suggest we move beyond just \"good intentions\" and implement structural changes:\n",
    "\n",
    "* **Ethical Analysis:** Teams should assume things will go wrong. Before building, ask:\n",
    "    * *\"Who could be hurt by this?\"*\n",
    "    * *\"What would a bad actor do with this?\"*\n",
    "* **Recourse:** There must be a \"human in the loop.\" If an AI denies someone a loan or flags them for fraud, there must be a way for that person to appeal the decision to a human.\n",
    "* **Diversity:** Hiring diverse teams is a safety feature. It increases the chance that someone in the room will spot a potential problem before the product ships.\n",
    "* **Regulation:** Just as the car industry resisted seatbelts until laws required them, the tech industry likely needs regulation to ensure safety.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40aeaa8-c202-4a3a-b144-d99161a15eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090ecdf-ffd3-4374-9514-3588c4bf0878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
